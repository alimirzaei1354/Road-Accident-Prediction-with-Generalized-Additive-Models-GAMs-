---
title: "Generalized Additive Models (GAMs)"
author: "Ali Mirzaei"
date: "11/4/2022"
output: html_document

## Fitting a GAM in R
```r
model <- gam(y ~ s(x1) + s(x2) + te(x3, x4), # formuala describing model
             data = my_data_frame,           # your data
             method = 'REML',                # or 'ML'
             family = gaussian)              # or something more exotic
```

`s()` terms are smooths of one or more variables

`te()` terms are the smooth equivalent of *main effects + interactions*

## Smooth interactions

Two ways to fit smooth interactions

1. Bivariate (or higher order) thin plate splines
    * `s(x, z, bs = 'tp')`
    * Isotropic; single smoothness parameter for the smooth
	* Sensitive to scales of `x` and `z`
2. Tensor product smooths
    * Separate marginal basis for each smooth, separate smoothness parameters
	* Invariant to scales of `x` and `z`
	* Use for interactions when variables are in different units
	* `te(x, z)`

## Tensor product smooths

There are multiple ways to build tensor products in *mgcv*

1. `te(x, z)`
2. `t2(x, z)`
3. `s(x) + s(z) + ti(x, z)`

`te()` is the most general form but not usable in `gamm4::gamm4()` or *brms*

`t2()` is an alternative implementation that does work in `gamm4::gamm4()` or *brms*

`ti()` fits pure smooth interactions; where the main effects of `x` and `z` have been removed from the basis

## Type of smooths

The type of smoother is controlled by the `bs` argument (think *basis*)

The default is a low-rank thin plate spline `bs = 'tp'`

Many others available



* Cubic splines `bs = 'cr'`
* P splines `bs = 'ps'`
* Cyclic splines `bs = 'cc'` or `bs = 'cp'`
* Adaptive splines `bs = 'ad'`
* Random effect `bs = 're'`
* Factor smooths `bs = 'fs'`
* Duchon splines `bs = 'ds'`
* Spline on the sphere `bs = 'sos'`
* MRFs `bs = 'mrf'`
* Soap-film smooth `bs = 'so'`
* Gaussian process `bs = 'gp'`

## Factor smooth interactions

Two ways for factor smooth interactions

1. `by` variable smooths
    * entirely separate smooth function for each level of the factor
	* each has it's own smoothness parameter
	* centred (no group means) so include factor as a fixed effect
	* `y ~ f + s(x, by = f)`
2. `bs = 'fs'` basis
    * smooth function for each level of the function
	* share a common smoothness parameter
	* fully penalized; include group means
	* closer to random effects
	* `y ~ s(x, f, bs = 'fs')`


## A bestiary of conditional distributions

A GAM is just a fancy GLM

Simon Wood & colleagues (2016) have extended the *mgcv* methods to some non-exponential family distributions


* `binomial()`
* `poisson()`
* `quasipoisson()`
* `Gamma()`
* `inverse.gaussian()`
* `nb()`
* `tw()`
* `mvn()`
* `multinom()`
* `betar()`
* `scat()`
* `gaulss()`
* `ziplss()`
* `twlss()`
* `cox.ph()`
* `gamals()`
* `ocat()`

## Libraries

```{r libraries, message=FALSE, warning=FALSE}
if(!require('dplyr')) install.packages('dplyr')
if(!require('corrplot')) install.packages('corrplot')
if(!require('mgcv')) install.packages('mgcv')
if(!require('plotly')) install.packages('plotly')
if(!require('GGally')) install.packages('GGally')
if(!require('gratia')) install.packages('gratia')
if(!require('ggeffects')) install.packages('ggeffects')
if(!require('scico')) install.packages('scico')
if(!require('beepr')) install.packages('beepr')
if(!require('purrr')) install.packages('purrr')
if(!require('tibble')) install.packages('tibble')
if(!require('visibly')) devtools::install_github('m-clark/visibly', upgrade = "never")
if(!require('tidyext')) devtools::install_github('m-clark/tidyext', upgrade = "never")
```
# Dataset
The data set has been constructed using  
traffic parameters of cars and police enforcments on the road level(Independent variables) and Fatal and injured accidents as dependent variable(Y) .The Independent variables are as follows:
- Speed violation
- Distance violation
Volum of all kinds of classes of vehicles included:
- Class one: light cars
- Class 2: Semi-heavy vehicles
- Class 3: heavy vehicles
- Class 4: Bus
- Class five: super_heavy vehicles
- The Volum of police enforment on the road


```{r dataset, echo=FALSE, warning=FALSE}
The first thing to do is get the data in and do some initial inspections.
df <- read.csv(paste0(here::here(),'/mydata.csv'), encoding="UTF-8")

df <- df %>% mutate_if(is.character, as.factor) %>% 
  relocate(accident)
# mydata have 4 class of days:high_holiday=="long vacation or special dayes",low_holiday=="short vacation",holiday=="only friday",normal=="No holidays"
df$high_holiday <- ifelse(df$TAG ==  "تعطيلي بلند مدت", 1 , 0) #long vacation 
df$low_holiday <- ifelse(df$TAG ==  "تعطيلي کوتاه مدت", 1 , 0) #short vacation
df$holiday <- ifelse(df$TAG ==  "تعطيل", 1 , 0) #only friday
df$normal <- ifelse(df$TAG ==  "عادي", 1 , 0) #No holidays 
#Another feature of the days of the week is IS_HOLIDAY=="holidays or no holidays"
cols <- c('IS_HOLIDAY'
           , 'high_holiday', 'low_holiday', 'holiday', 'normal'
          )
df[cols] <- lapply(df[cols], factor)
# df <- df %>% mutate(c.day = 1:nrow(df))
```
# glimpse makes it possible to see every column in a data frame
```{r data}
glimpse(df)
```
### Exploratory Data Analysis
We can look at the individual relationships of covariates with overall science score.
```{r pairs_plot}
smooth <- function(data, mapping, ptcol, ptalpha=1, ptsize=1, linecol, ...) {
p <- ggplot(data = data, mapping = mapping) +
geom_point(color=ptcol, alpha=ptalpha, size=ptsize) +
geom_smooth(color=linecol, ...)
p
}
ggpairs(df)
```
## Interactions between  variables
Splitting up continuous variables is generally a bad idea. In terms of statistical efficiency, the popular practice of dichotomising continuous variables at their median is comparable to throwing out a third of the dataset. Moreover, statistical models based on split-up continuous variables are prone to misinterpretation: threshold effects are easily read into the results when, in fact, none exist. Splitting up, or ‘binning’, continuous variables, then, is something to avoid. But what if you’re interested in how the effect of one continuous predictor varies according to the value of another continuous predictor? In other words, what if you’re interested in the interaction between two continuous predictors? Binning one of the predictors seems appealing since it makes the model easier to interpret.
source:https://janhove.github.io/analysis/2017/06/26/continuous-interactions
##  Exploratory plots
```{r Exploratory plots}
par(mfrow = c(1, 3))

# Intercorrelation DISTANCE_VIOLATIONS/SPEED_VIOLATIONS
# First, let’s draw some quick graphs to get a sense of how the predictors and the accident are related.
plot(SPEED_VIOLATIONS ~ DISTANCE_VIOLATIONS, df)
lines(lowess(df$DISTANCE_VIOLATIONS, df$SPEED_VIOLATIONS), col = "red")

# SPEED_VIOLATIONS vs. accident
plot(accident ~ SPEED_VIOLATIONS, df)
lines(lowess(df$SPEED_VIOLATIONS, df$accident), col = "red")

# DISTANCE_VIOLATIONS vs. accident
plot(accident ~ DISTANCE_VIOLATIONS, df)
lines(lowess(df$DISTANCE_VIOLATIONS, df$accident), col = "red")
```
# conditioning plots
When we suspect there’s an interaction between two predictors, it’s useful to draw conditioning plots as well. For these plots, the dataset is split up into a number of overlapping equal-sized regions defined by a conditioning variable, and the relationship between the predictor of interest and the outcome within each region is plotted.Our assumption is that road accidents have a high impact of SPEED_violations &DISTANCE_violations.
```{r conditioning plots}
library(lattice)
coplot(accident ~ SPEED_VIOLATIONS | DISTANCE_VIOLATIONS, data = df, 
       number = 4, rows = 1,
       panel = panel.smooth)
```
# Mode0 (Linear Fit)

We will start with the simple situation of a single predictor. Let's begin by using a typical linear regression to predict science scores by the Income index.  We could use the standard R `lm` function, but I'll leave that as an exercise for comparison.  We can still do straightforward linear models with the `gam` function, and again it is important to note that the standard linear model can be seen as a special case of a GAM.

```{r mod_lm, echo=-4}
library(mgcv)
mod_lm <- lm(accident ~  SPEED_VIOLATIONS, data=df)
summary(mod_lm)
```
# factor variables
Notice how R reports an intercept parameter and parameters for the two treatment
levels, but, in order to obtain an identifiable model, it has not included a parameter
for the control level of the group factor.
TAG is a factor variable in my data.
df$TAG <- as.factor(df$TAG)
fit <- lm(accident ~ TAG , data=df)
summary(fit)
par(mfrow=c(3,3))
plot(fit)
#Measures of Influence
olsrr offers the following tools to detect influential observations:
-Cook’s D Bar Plot
-Cook’s D Chart
-DFBETAs Panel
-DFFITs Plot
-Studentized Residual Plot
-Standardized Residual Chart
-Studentized Residuals vs Leverage Plot
-Deleted Studentized Residual vs Fitted Values Plot
-Hadi Plot
-Potential Residual Plot

ols_plot_cooksd_bar(mod_lm)  # A data point having a large cook’s d indicates that the data point strongly influences the fitted values.
###
### GAM
### Fitting the model
# Model 1

```{r smooth_parameter, fig.width=10, warning=FALSE}
fits = purrr::map_df(
  c(0.00001, 0.01, 1, 100, 1000000),
  function(p)
    tibble(polynomial = p,
           JDATE = df$JDATE,
           fits = fitted(
             gam(acc_total ~ s(SPEED_VIOLATIONS, k = 30) +
                   s(DISTANCE_VIOLATIONS) 
                 ,data = df, family = 'poisson', sp = p)
           ))
) %>%
  mutate(polynomial = factor(polynomial, labels = c(0.00001, 0.01, 1, 100, 10000),))



plot_ly(data = df) %>%
  add_markers( x = ~JDATE,
               y = ~acc_total,
               marker = list(color = '#D55E00', opacity = .2),
               showlegend = F) %>%
  add_lines( x = ~JDATE, y = ~ fits, color =  ~ polynomial, data = fits)%>%
  theme_plotly() %>% config(displayModeBar = T)
```




